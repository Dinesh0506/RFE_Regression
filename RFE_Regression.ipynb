{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29986e0-b88a-4cf5-a793-6208d0b4fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the important libraries\n",
    "#data manipulation and analysis\n",
    "import pandas as pd\n",
    "#dataset spliting purpose using the libary train_test_split\n",
    "from sklearn.model_selection import train_test_split \n",
    "#mearsure the execution time\n",
    "import time\n",
    "#Numerical computing and handle large,multi-dimensional dataset\n",
    "import numpy as np\n",
    "#Feature Scaling for data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Feature Selecting for machine learning\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "#features that have the strongest relationship with the target variable\n",
    "from sklearn.feature_selection import chi2\n",
    "#Feature Selecting for machine learning\n",
    "from sklearn.feature_selection import RFE\n",
    "#perform the linear model into logisticregression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# serializing and deserializing Python objects and process of converting a Python object into a byte stream-pickle\n",
    "import pickle\n",
    "#creating static, animated, and interactive visualizations.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#input and output split the dataset using function\n",
    "def split_scalar(indep_X,dep_Y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(indep_X, dep_Y, test_size = 0.25, random_state = 0)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(indep_X,dep_Y, test_size = 0.25, random_state = 0)\n",
    "        \n",
    "        #Feature Scaling\n",
    "        #from sklearn.preprocessing import StandardScaler\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)    \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "#Calculating the r2 score for each model as define\n",
    "def r2_prediction(regressor,X_test,y_test):\n",
    "     y_pred = regressor.predict(X_test)\n",
    "     from sklearn.metrics import r2_score\n",
    "     r2=r2_score(y_test,y_pred)\n",
    "     return r2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3419ba2e-22ac-4dbc-97cd-87d30fc19412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the linear model regression\n",
    "def Linear(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        regressor = LinearRegression()\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2   \n",
    "\n",
    "#Defining the SVM Linear Regression \n",
    "def svm_linear(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVR\n",
    "        regressor = SVR(kernel = 'linear')\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2  \n",
    "\n",
    "#Defining the Svm Non Linear Regression\n",
    "def svm_NL(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVR\n",
    "        regressor = SVR(kernel = 'rbf')\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2  \n",
    "     \n",
    "#Defining the Decision tree Regression\n",
    "def Decision(X_train,y_train,X_test):\n",
    "        \n",
    "        # Fitting K-NN to the Training setC\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        regressor = DecisionTreeRegressor(random_state = 0)\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2  \n",
    "     \n",
    "#Defining the Random Forest Regression\n",
    "def random(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49168ab6-94d6-40b5-ba8d-c01e9727515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing the Recursive Feature Elimination on each regression model    \n",
    "def rfeFeature(indep_X,dep_Y,n):\n",
    "        rfelist=[]\n",
    "        # List of models to use for RFE\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        lin = LinearRegression()\n",
    "        \n",
    "        from sklearn.svm import SVR\n",
    "        SVRl = SVR(kernel = 'linear')\n",
    "        \n",
    "        from sklearn.svm import SVR\n",
    "        SVRnl = SVR(kernel = 'rbf')\n",
    "        \n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        dec = DecisionTreeRegressor(random_state = 0)\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        rf = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "        rfemodellist=[lin,SVRl,SVRnl,dec,rf] \n",
    "        \n",
    "        for i in   rfemodellist:\n",
    "            print(i)\n",
    "            log_rfe = RFE(i,n)\n",
    "            log_fit = log_rfe.fit(indep_X, dep_Y)\n",
    "            log_rfe_feature=log_fit.transform(indep_X)\n",
    "            rfelist.append(log_rfe_feature)\n",
    "        return rfelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b4fbc6-a666-4cd7-bcab-a6555d671195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe the Recursive Feature Elimination on each regression model\n",
    "def rfe_regression(log,svmnl,svml,des,rf): \n",
    "    \n",
    "    rfedataframe=pd.DataFrame(index=['Linear','SVR','SVC','Random','DecisionTree'],columns=['Linear','SVMnl','SVMl',\n",
    "                                                                                        'Decision','Random'])\n",
    "\n",
    "    for number,idex in enumerate(rfedataframe.index):\n",
    "        \n",
    "        rfedataframe['Linear'][index]=log[number]\n",
    "        rfedataframe['SVMnl'][index]=svmnl[number]\n",
    "        rfedataframe['SVMl'][index]=svml[number]\n",
    "        rfedataframe['Decision'][index]=des[number]\n",
    "        rfedataframe['Random'][index]=rf[number]\n",
    "    return rfedataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b136d50-0a60-4af2-a26d-002ecad3bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RFE.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m dep_Y\u001b[38;5;241m=\u001b[39mdf2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_yes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#RFM with i/p and o/p with feature selection\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m rfelist \u001b[38;5;241m=\u001b[39m \u001b[43mrfeFeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindep_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdep_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m       \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Empty list created for each model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m lin\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mrfeFeature\u001b[1;34m(indep_X, dep_Y, n)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m   rfemodellist:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 23\u001b[0m     log_rfe \u001b[38;5;241m=\u001b[39m \u001b[43mRFE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     log_fit \u001b[38;5;241m=\u001b[39m log_rfe\u001b[38;5;241m.\u001b[39mfit(indep_X, dep_Y)\n\u001b[0;32m     25\u001b[0m     log_rfe_feature\u001b[38;5;241m=\u001b[39mlog_fit\u001b[38;5;241m.\u001b[39mtransform(indep_X)\n",
      "\u001b[1;31mTypeError\u001b[0m: RFE.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#read the data set from csv file\n",
    "dataset1=pd.read_csv(\"prep.csv\",index_col=None)\n",
    "df2=dataset1\n",
    "#first covert categorical value in to dummies then it can be drop the first categorical data \n",
    "df2 = pd.get_dummies(df2, drop_first=True)\n",
    "#Drop the classification_yes Column\n",
    "indep_X=df2.drop('classification_yes', axis=1)\n",
    "#Dependent variable\n",
    "dep_Y=df2['classification_yes']\n",
    "\n",
    "#RFM with i/p and o/p with feature selection\n",
    "rfelist = rfeFeature(indep_X,dep_Y,5)       \n",
    "#Empty list created for each model\n",
    "lin=[]\n",
    "svml=[]\n",
    "svmnl=[]\n",
    "des=[]\n",
    "rf=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a78d3-09be-4c92-bde9-ec842c207041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing the each model r2score value will add to the empty list of each model\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_Y)  \n",
    "    r2_lin=Linear(X_train,y_train,X_test)\n",
    "    lin.append(r2_lin)\n",
    "    \n",
    "    r2_sl=svm_linear(X_train,y_train,X_test)    \n",
    "    svml.append(r2_sl)\n",
    "    \n",
    "    r2_NL=svm_NL(X_train,y_train,X_test)\n",
    "    svmnl.append(r2_NL)\n",
    "    \n",
    "    r2_d=Decision(X_train,y_train,X_test)\n",
    "    des.append(r2_d)\n",
    "    \n",
    "    r2_r=random(X_train,y_train,X_test)\n",
    "    rf.append(r2_r)\n",
    "    \n",
    "    \n",
    "result=rfe_regression(lin,svml,des,rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51312a12-3952-4674-beb6-cb5add4b8567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
